
import urllib
mak_limit=5
def get_page(url):
  try:
		f = urllib.urlopen(url)
		page = f.read()
		f.close()
		return page
	except:
		return ""
		
def union(a,b):
	for e in b:
		if e not in a:
			a.append(e)
			
def get_next_url(page):
	start_link=page.find("a href")
	if(start_link==-1):
		return None,0
	start_quote=page.find('"',start_link)
	end_quote=page.find('"',start_quote+1)
	url=page[start_quote+1:end_quote]
	return url,end_quote

def get_all_links(page):
	links=[]
	while True:
		url,eposition=get_next_url(page)
		page=page[eposition:]
		if url:
			links.append(url)
		else:
			break
	return links

def Look_up(inx, keywrd):
	if keywrd in inx:
		return inx[keywrd]
	else:
		return None
	
def add_to_inx(inx,url,keywrd):
        if keywrd in inx:
                if url not in inx[keywrd]:
                        inx[keywrd].append(url)
                return
        inx[keywrd]=[url]
		
def add_page_to_inx(inx, url, content):
	words = content.split()
	for word in words:
		add_to_inx(inx, url,word)
	
def compute_ranks(graph):
	d=0.8
	numloops=20
	ranks={}
	npages=len(graph)
	for page in graph:
		ranks[page]=1.0/npages
	for i in range(0,numloops):
		newranks={}
		for page in graph:
			newrank=(1-d)/npages
			for node in graph:
				if page in graph[node]:
					newrank=newrank + d * (ranks[node]/len(graph[node]))
			newranks[page]=newrank
		ranks=newranks
	return ranks

def crawl_web(seed):
	tocrawl=[seed]
	crawled =[]
	inx ={}
	graph={}
	global max_limit
	while tocrawl:
		page=tocrawl.pop()
		if page not in crawled:
			max_limit-=1
			if max_limit<=0:
				break
			content=get_page(page)
			add_page_to_inx(inx,page,content)
			union(tocrawl,get_all_links(content))
			graph[page]=get_all_links(content)
			crawled.append(page)
	return crawled,inx,graph

def Look_up_new(inx,ranks,keywrd):
	pages=Look_up(inx,keywrd)
	for i in pages:
		print  '\n Results :\n'+i+" --> "+str(ranks[i])
	print "\nResult After Sorting  with Page Rank :\n"
	it=0
	for i in pages:
		it+=1
		print str(it)+'.\t'+i+'\n' 
	
print "Enter seed page :"
seed_page=raw_input()
print "Enter the Keyterm :"
search_term=raw_input()
try:
	print "Enter the Max Limit to Search :"
	max_limit=int(raw_input())
except:
	f=None
print '\nCrawling...'
crawled,inx,graph=crawl_web(seed_page)
ranks=compute_ranks(graph)
Look_up_new(inx,ranks,search_term)

